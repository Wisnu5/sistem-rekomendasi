# -*- coding: utf-8 -*-
"""Sistem_Rekomendasi.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sDXYcksB1L8R4C50sq7YF2L4tWj8Y5En
"""

# import library
import pandas as pd
import kagglehub
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import os
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from zipfile import ZipFile
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras import regularizers
from pathlib import Path

# Download latest version
path = kagglehub.dataset_download("aprabowo/indonesia-tourism-destination")
print("Path to dataset files:", path)

for filename in os.listdir(path):
    print(filename)

# data user
user = pd.read_csv(path + "/user.csv")
user.head()

user.shape

user.isnull().sum()

# data rating
tour_rate = pd.read_csv(path + "/tourism_rating.csv")
tour_rate.head()

tour_rate.shape

tour_rate.isnull().sum()

# data pack tour
tour_pack = pd.read_csv(path + "/package_tourism.csv")
tour_pack.head()

tour_pack.shape

# data tour with id
tour_info = pd.read_csv(path + "/tourism_with_id.csv")
tour_info.head()

tour_info.shape

tour_info.info()

tour_info.isnull().sum()

tour_info.duplicated().sum()

tour_info['Category'].unique()

tour_info['City'].unique()

print('Jumlah userID: ', len(tour_rate.User_Id.unique()))
print('Jumlah placeID: ', len(tour_rate.Place_Id.unique()))
print('Jumlah data rating: ', len(tour_rate))

"""## Ringkasan Eksplorasi Dataset Pariwisata Indonesia

Dataset ini terdiri dari **4 file utama** yang berisi informasi terkait destinasi wisata dan interaksi pengguna:

- `user.csv` berisi **300 pengguna**, masing-masing memiliki informasi **lokasi dan usia** tanpa nilai kosong.
- `tourism_rating.csv` memuat **10.000 data rating** dari pengguna terhadap tempat wisata, mencakup **300 User_Id** dan **437 Place_Id**, tanpa missing values.
- `package_tourism.csv` berisi **100 paket wisata** yang merekomendasikan hingga 5 destinasi wisata per kota. Beberapa kolom destinasi memiliki nilai `NaN` karena tidak semua paket memiliki 5 tempat.
- `tourism_with_id.csv` menyediakan informasi lengkap tentang **437 destinasi wisata**, termasuk:
  - Nama tempat, deskripsi, kategori, kota, harga tiket, rating, durasi kunjungan, serta koordinat geografis.
  - Kolom `Time_Minutes` memiliki banyak nilai kosong (**232 missing**).
  - Kolom `Unnamed: 11` tidak memiliki data sama sekali dan bisa dihapus.
  - Terdapat **6 kategori wisata**:
    - Budaya
    - Taman Hiburan
    - Cagar Alam
    - Bahari
    - Pusat Perbelanjaan
    - Tempat Ibadah
  - Kota yang tercakup dalam data:
    - Jakarta
    - Yogyakarta
    - Bandung
    - Semarang
    - Surabaya

- Dataset dalam kondisi **bersih**, tidak ada duplikasi atau nilai kosong signifikan (kecuali pada tour_info).

# Data Preprocessing
"""

# menggabungkan place id tour_info dengan place id tour_rate
info_tourism = tour_info[["Place_Id", "Place_Name", "Description", "City", "Category"]]
all_tourism_rate = tour_rate
df = pd.merge(all_tourism_rate, info_tourism, on='Place_Id', how='left')
df.head()

"""Data `tour_rate` yang berisi rating pengguna digabungkan dengan `tour_info` yang berisi informasi lengkap tempat wisata. Ini menghasilkan dataset `df` yang menyatukan data rating, nama tempat, deskripsi, kota, dan kategori wisata.

"""

# menggabungkan city dengan categori
df['City_Category'] = df[['City','Category']].agg(' '.join, axis=1)
df.head()

"""Dibuat fitur baru `City_Category` yang menggabungkan kota dan kategori wisata. Fitur ini dapat membantu analisis atau rekomendasi berdasarkan kombinasi lokasi dan jenis wisata."""

df.isnull().sum()

"""Hasil pengecekan menunjukkan bahwa tidak ada nilai kosong pada dataset `df`, sehingga data siap untuk digunakan dalam analisis lebih lanjut tanpa perlu imputasi."""

# Membuang data duplikat pada variabel preparation
preparation = df.copy()
preparation = preparation.drop_duplicates('Place_Id')
preparation.head()

"""Data `df` masih memiliki duplikasi tempat wisata berdasarkan `Place_Id`, sehingga dilakukan deduplikasi untuk memperoleh data tempat wisata unik sebanyak satu entri per lokasi."""

# mengonversi data series 'Place_Id' dalam bentuk list
place_id = preparation['Place_Id'].tolist()

# mengonversi data series 'Place_Name' dalam bentuk list
place_name = preparation['Place_Name'].tolist()

# mengonversi data series 'Category' dalam bentuk list
place_category = preparation['Category'].tolist()

# mengonversi data 'Description' dalam bentuk list
place_desc = preparation['Description'].tolist()

# mengonversi data series 'City' dalam bentuk list
place_city = preparation['City'].tolist()

# mengonversi data series 'City_Category' dalam bentuk list
city_category = preparation['City_Category'].tolist()

print(len(place_id))
print(len(place_name))
print(len(place_category))
print(len(place_desc))
print(len(place_city))
print(len(city_category))

"""Kolom-kolom utama seperti ID, nama, deskripsi, kota, dan kategori disimpan dalam bentuk list untuk mempermudah pembuatan dataframe baru yang unik dan lebih ringkas."""

# membuat dictionary untuk data 'place_id', 'place_name', 'place_category', 'place_desc', 'place_city', 'city_category'
df_baru = pd.DataFrame({
    'Place_Id': place_id,
    'Place_Name': place_name,
    'Category': place_category,
    'Description': place_desc,
    'City': place_city,
    'City_Category': city_category
})

# menampilkan data
df_baru.head()

"""Dibuat dataframe baru `df_baru` yang berisi hanya satu entri untuk tiap tempat wisata, dengan informasi nama, deskripsi, kota, dan kategorinya. Dataset ini berguna untuk sistem rekomendasi atau eksplorasi lebih lanjut."""

# menghitung jumlah kunjungan untuk setiap tempat wisata
place_counts = df['Place_Name'].value_counts()

# mengambil 10 tempat wisata dengan jumlah kunjungan terbanyak
top_10_places = place_counts.head(10)

# menampilkan 10 tempat wisata yang paling sering dikunjungi
print("10 tempat wisata yang paling sering dikunjungi:")
print(top_10_places)

# visualisasi
plt.figure(figsize=(12, 6))
sns.barplot(x=top_10_places.values, y=top_10_places.index)
plt.xlabel('Nama Tempat Wisata')
plt.ylabel('Jumlah Kunjungan')
plt.title('10 Tempat Wisata Paling Sering Dikunjungi')
plt.tight_layout()
plt.show()

"""Dihitung jumlah kunjungan untuk masing-masing tempat wisata, lalu diambil 10 besar yang paling banyak dikunjungi pengguna berdasarkan data rating.
Visualisasi menunjukkan tempat wisata yang paling populer di kalangan pengguna, yang bisa menjadi prioritas dalam sistem rekomendasi atau promosi.
"""

# menghitung jumlah kunjungan untuk setiap kategori tempat wisata
category_counts = df['Category'].value_counts()

# menampilkan jumlah kunjungan per kategori
print("\nJumlah kunjungan per kategori tempat wisata:")
print(category_counts)

# visualisasi
plt.figure(figsize=(12, 6))
sns.barplot(x=category_counts.values, y=category_counts.index)
plt.xlabel('Jumlah Kunjungan')
plt.ylabel('Kategori Tempat Wisata')
plt.title('Jumlah Kunjungan per Kategori Tempat Wisata')
plt.tight_layout()
plt.show()

"""Kunjungan dibagi berdasarkan kategori tempat wisata (taman hiburan, budaya, cagar alam, bahari, pusat pembelanjaan, tempat ibadah)"""

# distribusi usia
plt.figure(figsize=(5,3))
sns.boxplot(user['Age']);
plt.title('Distribusi Usia User', pad=20)
plt.show()

"""Visualisasi boxplot menunjukkan bahwa sebagian besar pengguna berada dalam rentang usia produktif. Usia ini dapat digunakan untuk mengarahkan strategi personalisasi konten."""

print(user['Age'].describe())

# distibusi harga masuk tiap kota
plt.figure(figsize=(15, 8))
sns.boxplot(x='City', y='Price', data=tour_info)
plt.title('Distribusi Harga Masuk Tiap Kota', pad=20)
plt.xticks(rotation=90)
plt.show()

"""Harga tiket masuk tempat wisata bervariasi antar kota. Beberapa kota memiliki distribusi harga yang lebih tinggi, yang bisa menjadi pertimbangan dalam rekomendasi berbasis anggaran."""

print(tour_info['Price'].describe())

# Visualisasi asal kota user
askot = user['Location'].apply(lambda x : x.split(',')[0])
plt.figure(figsize=(8,6))
sns.countplot(y=askot)
plt.title('Jumlah Asal Kota dari User')
plt.show()

"""Mayoritas pengguna berasal dari beberapa kota besar, yang dapat dijadikan dasar segmentasi pasar atau targeting pengguna berdasarkan lokasi."""

print(user['Location'].value_counts())

df_baru.head()

df_baru.shape

"""# Model Development Content Based Filtering"""

data = df_baru
data.sample(5)

# insialisasi TfidfVectorizer
tf = TfidfVectorizer()

# melakukan perhitungan idf pada data 'City_Category'
tf.fit(data['City_Category'])

# mapping array dari fitur index integer ke fitur nama
tf.get_feature_names_out()

"""Inisialisasi TfidfVectorizer pada kolom 'City_Category', yang memuat kombinasi kota dan kategori wisata, seperti “Jakarta Taman Hiburan”."""

# melakukan fit lalu ditransformasikan ke bentuk matrix
tfidf_matrix = tf.fit_transform(data['City_Category'])

# mengecek ukuran matrix tfidf
tfidf_matrix.shape

"""Transformasi ke TF-IDF matrix menghasilkan matriks (437, 15), artinya ada 437 tempat dan 15 token unik (kota/kategori)."""

# mengubah vektor tf-idf dalam bentuk matriks dengan fungsi todense()
tfidf_matrix.todense()

"""Konversi TF-IDF menjadi DataFrame untuk eksplorasi nilai bobot tiap token pada tiap tempat."""

# membuat dataframe untuk melihat tf-idf matrix
# kolom diisi dengan City_Category
# baris diisi dengan nama place

pd.DataFrame(
    tfidf_matrix.todense(),
    columns=tf.get_feature_names_out(),
    index=data['Place_Name']
).sample(22, axis=1, replace=True).sample(10, axis=0)

# menghitung cosine similarity pada matrix tf-idf
cosine_sim = cosine_similarity(tfidf_matrix)
cosine_sim

"""Perhitungan Cosine Similarity antar tempat wisata berbasis City_Category."""

# membuat dataframe dari variabel cosine_sim dengan baris dan kolom berupa nama place
cosine_sim_df = pd.DataFrame(cosine_sim, index=data['Place_Name'], columns=data['Place_Name'])
print('Shape:', cosine_sim_df.shape)

# menampilkan cosine
cosine_sim_df.sample(5, axis=1).sample(10, axis=0)

"""Pembuatan Cosine Similarity DataFrame, tempat wisata sebagai index dan kolom, menghasilkan matriks (437, 437).

## Mendapatkan Rekomendasi
"""

def tourism_recommendations(place_name, similarity_data=cosine_sim_df, items=data[['Place_Name', 'Category', 'Description', 'City']], k=5):
    """
    Rekomendasi Tempat Wisata berdasarkan kemiripan dataframe
    Parameter:
    ---
    place_name : tipe data string (str)
                Nama Tempat Wisata (index kemiripan dataframe)
    similarity_data : tipe data pd.DataFrame (object)
                      Kesamaan dataframe, simetrik, dengan resto sebagai
                      indeks dan kolom
    items : tipe data pd.DataFrame (object)
            Mengandung kedua nama dan fitur lainnya yang digunakan untuk mendefinisikan kemiripan
    k : tipe data integer (int)
        Banyaknya jumlah rekomendasi yang diberikan
    ---
    Pada index ini, kita mengambil k dengan nilai similarity terbesar
    pada index matrix yang diberikan (i).
    """
    # Mengambil data dengan menggunakan argpartition untuk melakukan partisi secara tidak langsung sepanjang sumbu yang diberikan
    # Dataframe diubah menjadi numpy
    # Range(start, stop, step)
    index = similarity_data.loc[:,place_name].to_numpy().argpartition(
        range(-1, -k, -1))

    # Mengambil data dengan similarity terbesar dari index yang ada
    closest = similarity_data.columns[index[-1:-(k+2):-1]]

    # Drop place_name agar nama tempat wisata yang dicari tidak muncul dalam daftar rekomendasi
    closest = closest.drop(place_name, errors='ignore')
    return pd.DataFrame(closest).merge(items).head(k)

"""Fungsi `tourism_recommendations` memberikan rekomendasi tempat wisata yang paling mirip berdasarkan kemiripan fitur dalam dataframe, seperti kategori, deskripsi, dan kota.  
Pendekatan ini memungkinkan personalisasi rekomendasi yang lebih tepat, mengoptimalkan pengalaman pengguna di platform wisata digital yang semakin berkembang.  
Dengan teknik similarity matrix dan argpartition, fungsi ini cepat dan efisien untuk skala data yang besar.

"""

data[data['Place_Name'].eq('Air Mancur Menari')]

# mendapatkan rekomendasi place name yang mirip dengan 'Air Mancur Menari'
tourism_recommendations('Air Mancur Menari')

"""Rekomendasi tempat wisata yang muncul menunjukkan tempat dengan kategori serupa dan lokasi yang dekat, yaitu taman hiburan di Surabaya.  
Hal ini memperkuat validitas model dalam mengenali kesamaan konteks dan lokasi, mendukung pengembangan sistem rekomendasi wisata berbasis konteks lokal yang relevan untuk pengguna.  
Di masa depan, model ini bisa dikembangkan dengan data perilaku pengguna untuk meningkatkan akurasi rekomendasi.
"""

tourism_recommendations("Trans Studio Bandung")

"""Fungsi berhasil merekomendasikan tempat wisata bertema taman hiburan di Bandung yang memiliki kemiripan fitur dan lokasi dengan Trans Studio Bandung.  
Ini menandakan kemampuan model untuk mengidentifikasi alternatif wisata yang menarik bagi pengunjung berdasarkan kemiripan pengalaman yang ditawarkan.  
Ke depan, integrasi data real-time seperti ulasan pengguna dan rating dapat memperkuat sistem rekomendasi ini menjadi lebih adaptif dan responsif.

# Model Development dengan Collaborative Filtering
"""

dt = tour_rate
dt

"""Memuat dataset `tour_rate` ke dalam variabel `dt` dan menampilkan strukturnya. Dataset berisi kolom `User_Id`, `Place_Id`, dan `Place_Ratings` dengan 10.000 baris, yang merepresentasikan rating pengguna terhadap tempat wisata.

## Data Preparation

## Encoding
"""

# mengubah PlaceID menjadi list tanpa nilai yang sama
place_ids = dt['Place_Id'].unique().tolist()
print('list PlaceID: ', place_ids)

# melakukan encoding placeID
place_to_place_encoded = {x: i for i, x in enumerate(place_ids)}
print('encoded PlaceID : ', place_to_place_encoded)

# melakukan proses encoding angka ke placeID
place_encoded_to_place = {i: x for i, x in enumerate(place_ids)}
print('encoded angka ke PlaceID: ', place_encoded_to_place)

"""Mengubah kolom `Place_Id` menjadi daftar tanpa duplikasi menggunakan `unique().tolist()`. Daftar ini berisi 437 ID tempat unik yang akan digunakan untuk encoding.

Melakukan encoding `Place_Id` menjadi indeks numerik menggunakan dictionary comprehension. `place_to_place_encoded` memetakan ID tempat asli ke indeks (0 hingga 436), dan `place_encoded_to_place` memetakan indeks kembali ke ID asli untuk keperluan dekoding.
"""

# mengubah userID menjadi list tanpa nilai yang sama
user_ids = dt['User_Id'].unique().tolist()
print('list userID: ', user_ids)

# melakukan encoding userID
user_to_user_encoded = {x: i for i, x in enumerate(user_ids)}
print('encoded userID : ', user_to_user_encoded)

# melakukan proses encoding angka ke userID
user_encoded_to_user = {i: x for i, x in enumerate(user_ids)}
print('encoded angka ke userID: ', user_encoded_to_user)

"""Mengubah kolom `User_Id` menjadi daftar tanpa duplikasi menggunakan `unique().tolist()`. Daftar ini berisi 300 ID pengguna unik yang akan digunakan untuk encoding.

Melakukan encoding `User_Id` menjadi indeks numerik menggunakan dictionary comprehension. `user_to_user_encoded` memetakan ID pengguna asli ke indeks (0 hingga 299), dan `user_encoded_to_user` memetakan indeks kembali ke ID asli untuk keperluan dekoding.
"""

# mapping User_Id ke dataframe user
dt['user'] = dt['User_Id'].map(user_to_user_encoded)

# mapping Place_Id ke dataframe place
dt['place'] = dt['Place_Id'].map(place_to_place_encoded)

"""Menambahkan kolom `user` dan `place` ke DataFrame `dt` dengan memetakan `User_Id` dan `Place_Id` menggunakan dictionary encoding yang telah dibuat. Kolom ini berisi indeks numerik untuk pengguna dan tempat."""

# mendapatkan jumlah user
num_users = len(user_to_user_encoded)
print('Jumlah user: ', num_users)

# mendapatkan jumlah place
num_places = len(place_to_place_encoded)
print('Jumlah place: ', num_places)

# mengubah rating menjadi float
dt['Place_Ratings'] = dt['Place_Ratings'].values.astype(np.float32)

# nilai minimum rating
min_rating = min(dt['Place_Ratings'])
print('Nilai minimum rating: ', min_rating)

# nilai maksimum rating
max_rating = max(dt['Place_Ratings'])
print('Nilai maksimum rating: ', max_rating)

"""Menghitung jumlah pengguna unik (`num_users` = 300) dan jumlah tempat unik (`num_places` = 437) berdasarkan dictionary encoding. Informasi ini akan digunakan untuk menentukan dimensi layer embedding pada model.

Mengubah kolom `Place_Ratings` menjadi tipe data `float32` untuk kompatibilitas dengan model machine learning. Juga menghitung nilai minimum (1.0) dan maksimum (5.0) rating untuk normalisasi.

## Membagi Data untuk Training dan Validasi
"""

# mengacak dataset
dt = dt.sample(frac=1, random_state=42)
dt

"""Mengacak urutan baris dataset menggunakan `sample(frac=1, random_state=42)` untuk memastikan distribusi data yang acak dan mencegah bias selama pelatihan model."""

# membuat variabel x untuk percobaan
x = dt[['user', 'place']].values

# membuat variabel y untuk membuat rating dari hasil
y = dt['Place_Ratings'].apply(lambda x: (x - min_rating) / (max_rating - min_rating)).values

# membagi menjadi 80:20
train_indences = int(0.8 * dt.shape[0])
x_train, x_val, y_train, y_val = (
    x[:train_indences],
    x[train_indences:],
    y[:train_indences],
    y[train_indences:],
)

print(x, y)

"""Membuat variabel `x` yang berisi pasangan `user` dan `place` sebagai input, serta `y` yang berisi rating ternormalisasi (skala 0-1) menggunakan formula `(rating - min_rating) / (max_rating - min_rating)`. Dataset dibagi menjadi 80% data pelatihan dan 20% data validasi.

## Proses Training
"""

import tensorflow as tf
from tensorflow.keras import layers
from tensorflow.keras import regularizers

class RecommenderNet(tf.keras.Model):
    def __init__(self, num_users, num_places, embedding_size, **kwargs):
        super(RecommenderNet, self).__init__(**kwargs)
        self.num_users = num_users
        self.num_places = num_places  # Perbaiki penamaan agar konsisten
        self.embedding_size = embedding_size

        # Layer embedding untuk user
        self.user_embedding = layers.Embedding(
            num_users,
            embedding_size,
            embeddings_initializer='he_normal',
            embeddings_regularizer=regularizers.l2(1e-6)
        )
        self.user_bias = layers.Embedding(num_users, 1)  # Layer embedding user bias

        # Layer embedding untuk place
        self.place_embedding = layers.Embedding(
            num_places,
            embedding_size,
            embeddings_initializer='he_normal',
            embeddings_regularizer=regularizers.l2(1e-6)
        )
        self.place_bias = layers.Embedding(num_places, 1)  # Layer embedding place bias

    def call(self, inputs):
        user_vector = self.user_embedding(inputs[:, 0])  # Embedding user
        user_bias = self.user_bias(inputs[:, 0])  # Bias user
        place_vector = self.place_embedding(inputs[:, 1])  # Embedding place
        place_bias = self.place_bias(inputs[:, 1])  # Bias place

        # Menghitung dot product antara user dan place
        dot_user_place = tf.tensordot(user_vector, place_vector, axes=2)

        # Menambahkan bias
        x = dot_user_place + user_bias + place_bias

        # Aktivasi sigmoid untuk output
        return tf.nn.sigmoid(x)

"""Mendefinisikan kelas `RecommenderNet` menggunakan TensorFlow Keras untuk model sistem rekomendasi berbasis collaborative filtering. Model menggunakan embedding untuk pengguna dan tempat, serta bias untuk masing-masing, dengan output dihasilkan melalui dot product dan aktivasi sigmoid."""

model = RecommenderNet(num_users, num_places, 100)

# model compile
model.compile(
    loss = tf.keras.losses.BinaryCrossentropy(),
    optimizer = keras.optimizers.Adam(learning_rate=0.001),
    metrics=[tf.keras.metrics.RootMeanSquaredError()]
)

# memulai training
history = model.fit(
    x=x_train,
    y=y_train,
    batch_size=8,
    epochs=100,
    verbose=1,
    validation_data=(x_val, y_val)
)

"""Mengompilasi model dengan loss function `BinaryCrossentropy`, optimizer `Adam` (learning rate 0.001), dan metrik `RootMeanSquaredError`. Model dilatih selama 100 epoch dengan batch size 8, menggunakan data pelatihan dan validasi untuk memantau performa.

## Visualisasi Metrik
"""

plt.plot(history.history['root_mean_squared_error'])
plt.plot(history.history['val_root_mean_squared_error'])
plt.title('model_metrics')
plt.ylabel('root_mean_squared_error')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

"""Grafik menunjukkan perbandingan RMSE untuk data pelatihan (train) dan data uji (test) selama 100 epoch. RMSE data pelatihan menurun tajam di awal dan stabil di sekitar 0.32-0.33, menunjukkan model belajar dengan baik dari data pelatihan. RMSE data uji meningkat setelah epoch awal dan stabil di sekitar 0.35-0.36, mengindikasikan adanya overfit karena model tidak generalisasi dengan baik pada data uji.

## Mendapatkan Rekomendasi Tempat Wisata

Memuat dataset `tour_rate` ke variabel `df` dan dataset lain ke `resto_df` sebagai `df_baru` untuk digunakan dalam analisis dan rekomendasi tempat wisata.
Menerima input `user_id` dari pengguna (misalnya 40) untuk memfilter data berdasarkan pengguna tertentu dan menghasilkan rekomendasi tempat yang sesuai.
Mengambil daftar `Place_Id` yang telah dikunjungi oleh pengguna dengan `User_Id` tertentu dari dataset `df`, digunakan sebagai dasar untuk menentukan tempat yang belum dikunjungi.
Mencari `Place_Id` yang belum dikunjungi oleh pengguna dari `resto_df` dengan menghapus tempat yang sudah ada di `place_visited_by_user`. Hasil difilter agar hanya mencakup ID tempat yang ada dalam `place_to_place_encoded`.
Mengonversi daftar tempat yang belum dikunjungi menjadi array dengan menambahkan `user_encoder` (indeks pengguna yang sudah diencode) untuk setiap tempat, sehingga siap digunakan sebagai input untuk model prediksi.
Menggunakan model yang telah dilatih untuk memprediksi rating untuk setiap tempat yang belum dikunjungi oleh pengguna. Hasil prediksi disimpan dalam bentuk array yang diratakan (`flatten()`).
Mengurutkan indeks rating tertinggi (top 10) menggunakan `argsort()` dan mengonversi indeks tersebut kembali ke `Place_Id` menggunakan `place_encoded_to_place` untuk mendapatkan daftar rekomendasi.
"""

resto_df = df_baru
df = tour_rate

user_id = int(input("Enter user ID: "))
place_visited_by_user = df[df.User_Id == user_id]

place_not_visited = resto_df[~resto_df['Place_Id'].isin(place_visited_by_user.Place_Id.values)]['Place_Id']
place_not_visited = list(
    set(place_not_visited)
    .intersection(set(place_to_place_encoded.keys()))
)
place_not_visited = [[place_to_place_encoded.get(x)] for x in place_not_visited]
user_encoder = user_to_user_encoded.get(user_id)
user_place_array = np.hstack(
    ([[user_encoder]] * len(place_not_visited), place_not_visited)
)

ratings = model.predict(user_place_array).flatten()
top_ratings_indices = ratings.argsort()[-10:][::-1]
recommended_place_ids = [
    place_encoded_to_place.get(place_not_visited[x][0]) for x in top_ratings_indices
]

print('Showing recommendations for users: {}'.format(user_id))
print('===' * 9)
print('Place with high ratings from user')
print('----' * 8)
top_place_user = (
    place_visited_by_user.sort_values(
        by = 'Place_Ratings',
        ascending=False
    )
    .head(5)
    .Place_Id.values
)
place_df_rows = resto_df[resto_df['Place_Id'].isin(top_place_user)]
for row in place_df_rows.itertuples():
    print(row.Place_Name, ':', row.Category)
print('----' * 8)
print('Top 10 Place recommendation')
print('----' * 8)
recommended_place = resto_df[resto_df['Place_Id'].isin(recommended_place_ids)]
for row in recommended_place.itertuples():
    print(row.Place_Name, ':', row.Category)

"""Menampilkan 10 tempat yang direkomendasikan oleh model (misalnya Monumen Yogya Kembali, Jogja Bay Pirates Adventure Waterpark, dll.) beserta kategori tempatnya, berdasarkan prediksi rating tertinggi."""